name: Generate All Pok√©mon Set Sheets

on:
  schedule:
    - cron: "0 12 * * *" # Runs daily at 12:00 UTC
  workflow_dispatch:

jobs:
  discover-sets:
    runs-on: ubuntu-latest
    outputs:
      sets: ${{ steps.get_sets.outputs.matrix }}
    steps:
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"
      - name: Install Dependencies and Browsers
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 playwright
          python -m playwright install --with-deps
      - name: Discover All Sets from TCGplayer
        id: get_sets
        run: |
          python << 'EOF'
          from bs4 import BeautifulSoup
          from playwright.sync_api import sync_playwright
          import json
          import os

          URL = "https://www.tcgplayer.com/search/pokemon/product?productLineName=pokemon&productTypeName=Cards&view=grid"
          
          print(f"Discovering sets from: {URL}")

          html_content = ""
          try:
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True)
                  page = browser.new_page()
                  page.goto(URL, wait_until="networkidle", timeout=60000)
                  html_content = page.content()
                  browser.close()
          except Exception as e:
              print(f"!!! FAILED to discover sets: {e}")
              exit(1)

          soup = BeautifulSoup(html_content, "html.parser")
          set_list = []
          
          # FIXED: Updated the selector to find the set filter dropdown by its 'name' attribute.
          set_filter = soup.find("select", {"name": "setName"})
          if not set_filter:
              print("!!! Could not find the set filter dropdown on the page. TCGplayer has likely changed their HTML again.")
              exit(1)

          for option in set_filter.find_all("option"):
              set_url_name = option.get("value")
              set_display_name = option.text.strip()
              
              if set_url_name and set_display_name:
                  filter_name = set_display_name.split(': ')[-1].split(' ')[0]
                  
                  set_list.append({
                      "set_url_name": set_url_name,
                      "set_filter_name": filter_name,
                      "set_display_name": set_display_name
                  })

          json_output = json.dumps(set_list)
          print(f"Discovered {len(set_list)} sets.")
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as hf:
              print(f'matrix={json_output}', file=hf)
          EOF

  generate-sheets:
    needs: discover-sets
    permissions:
      contents: write
    runs-on: ubuntu-latest
    strategy:
      matrix:
        set: ${{ fromJson(needs.discover-sets.outputs.sets) }}
      fail-fast: false

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install Dependencies and Browsers
        run: |
          python -m pip install --upgrade pip
          pip install pandas requests beautifulsoup4 Pillow fpdf2 replicate playwright
          python -m playwright install --with-deps
          
      - name: Scrape TCGplayer Data for ${{ matrix.set.set_display_name }}
        env:
          SET_URL_NAME: ${{ matrix.set.set_url_name }}
          SET_FILTER_NAME: ${{ matrix.set.set_filter_name }}
        run: |
          python << 'EOF'
          import pandas as pd
          from bs4 import BeautifulSoup
          import re
          from playwright.sync_api import sync_playwright
          import time
          import os

          set_url_name = os.environ.get('SET_URL_NAME')
          TARGET_SET_NAME = os.environ.get('SET_FILTER_NAME')
          
          BASE_URL = f"https://www.tcgplayer.com/search/pokemon/{set_url_name}?productLineName=pokemon&setName={set_url_name}&productTypeName=Cards&view=grid"
          CSV_FILENAME = f"{set_url_name}.csv"
          
          all_cards_list = []
          page_number = 1

          print(f"Starting scraper for set containing '{TARGET_SET_NAME}'...")

          with sync_playwright() as p:
              browser = p.chromium.launch(headless=True)
              page = browser.new_page(viewport={"width": 1920, "height": 1080})

              while True:
                  paginated_url = f"{BASE_URL}&page={page_number}"
                  print(f"Scraping URL: {paginated_url}")
                  
                  html_content = ""
                  try:
                      page.goto(paginated_url, wait_until="networkidle", timeout=60000)
                      time.sleep(3)
                      html_content = page.content()
                  except Exception as e:
                      print(f"Browser operation finished or failed on page {page_number}: {e}")
                      break

                  soup = BeautifulSoup(html_content, "html.parser")
                  search_results = soup.find_all("div", class_="search-result")

                  if not search_results:
                    print(f"No more search results found on page {page_number}. Ending scrape.")
                    break
                  
                  print(f"Found {len(search_results)} card results on page {page_number}. Now parsing and filtering...")

                  for card in search_results:
                      name_tag = card.find("span", class_="product-card__title")
                      price_tag = card.find("span", class_="product-card__market-price--value")
                      image_tag = card.find("img")
                      set_name_tag = card.find("h4", class_="